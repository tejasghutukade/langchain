{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development with Langchain\n",
    "\n",
    "## <img src=\"https://python.langchain.com/v0.1/img/brand/wordmark.png\" alt=\"Langchain\" width=\"200\"/> Langchain\n",
    "\n",
    "### Langchain is open source orchestration framework for the development of applications that use large language models\n",
    "\n",
    "\n",
    "1. LLM : \n",
    "    Large Language Models (LLMs) are a core component of LangChain. LangChain does not serve its own LLMs, but rather provides a standard interface for interacting with many different LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting started with connecting to an LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain langchain-core langchain-community langchain-openai langchain-chroma langchainhub tavily-python sqlalchemy==1.4.46 snowflake-sqlalchemy snowflake-connector-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "# pass in your openai api key as environment variable OPENAI_API_KEY\n",
    "openai_model = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = openai_model.invoke(\"what are the SOLID principles in programming?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locla LLM\n",
    "For local llm download [Ollama](https://ollama.com/) and after installation run \n",
    "`ollama run mistral` which will expose the model on http://localhost:11434\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "mistral_model = Ollama(model=\"mistral\")\n",
    "llama3_model = Ollama(model=\"llama3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = mistral_model.invoke(\"what are the SOLID principles in programming?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llama3_model.invoke(\"what are the SOLID principles in programming?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompts\n",
    "Prompts are set of instructions given to a large language model. The prompt template class in langchain formalizes the composition of prompt without the need to manually hardcode context and query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Generate me a list of {count} catchy and unique domain names for a {business}\",\n",
    "    input_variables=[\"count\", \"business\"]\n",
    ")\n",
    "\n",
    "result = openai_model.invoke(prompt.invoke({\"count\": 7, \"business\": \"Shawarma Joint\"}))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompts with Structured output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "\n",
    "# Define your desired data structure.\n",
    "class DomainNameList(BaseModel):\n",
    "    domain_names: list[str] = Field(description=\"A list of domain names .\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=DomainNameList)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Generate me a list of {count} domain names for a {business} . output format: {format_instructions}\",\n",
    "    input_variables=[\"count\", \"business\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | openai_model | parser\n",
    "output = chain.invoke({\"count\": 6,\"business\": \"Chipotle joint\"})\n",
    "print(output)\n",
    "print(type(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "Every llm support embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "openai_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "mistral_embeddings = OllamaEmbeddings(model=\"mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = openai_embeddings.embed_query(\"what are the SOLID principles in programming?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexes / Vector databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already ran the below code to generate the embeddings\n",
    "import os\n",
    "from langchain_community.document_loaders import ConfluenceLoader\n",
    "\n",
    "loader = ConfluenceLoader(\n",
    "    url=\"https://accountname.atlassian.net/wiki\", username=\"username\", api_key=os.environ[\"ATLASSIAN_API_KEY\"],\n",
    "    space_key=\"confluence_space\",\n",
    "    include_attachments=False,\n",
    "    limit=5\n",
    ")\n",
    "# documents = loader.load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "db = Chroma(persist_directory=\"./chroma_db\", embedding_function=mistral_embeddings,collection_name=\"confluence_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.similarity_search(\"What is Resources are available?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Augmented Generation (Rag) \n",
    "RAG is a technique for augmenting LLM knowledge with additional data. \n",
    "LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on.\n",
    "If you want to build AI applications that can reason about private data or data introduced after a model's cutoff date, you need to augment the knowledge of the model with the specific information it needs. \n",
    "The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG)\n",
    "\n",
    "#### Performing RAG on confluence documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
    "for message in retrieval_qa_chat_prompt.messages:\n",
    "    print(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "combine_docs_chain = create_stuff_documents_chain(\n",
    "    mistral_model, retrieval_qa_chat_prompt\n",
    ")\n",
    "retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = retrieval_chain.invoke({\"input\": \"What tools and resources are available in digital partnerships space? Give a list of key tools and resources.\"})\n",
    "# what is vista x wix plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in result:\n",
    "    if isinstance(result[message], list):\n",
    "        print(message + \": \" + ', '.join(map(str, result[message])))\n",
    "    else:\n",
    "        print(message + \": \" + str(result[message]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agents and tools\n",
    "The core idea of agents is to use a language model to choose a sequence of actions to take. In chains, a sequence of actions is hardcoded (in code). In agents, a language model is used as a reasoning engine to determine which actions to take and in which order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.agents import AgentExecutor, create_openai_functions_agent\n",
    "from langchain_openai import ChatOpenAI # Need a model with chat capabilities\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0) # Intialize the chat LLM\n",
    "# Define the instructions for the agent\n",
    "instructions = \"\"\"You are an assistant.\"\"\"\n",
    "base_prompt = hub.pull(\"langchain-ai/openai-functions-template\")\n",
    "prompt = base_prompt.partial(instructions=instructions)\n",
    "\n",
    "# Define the tools\n",
    "tool = TavilySearchResults() # Search engine tool\n",
    "tavily_tool = TavilySearchResults()\n",
    "tools = [tavily_tool]\n",
    "\n",
    "# Create the agent\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "\n",
    "# Create the agent executor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent_executor.invoke({\"input\": \"Whats latest in react js 19?\"})\n",
    "agent_executor.invoke({\"input\": \"What is the weather in Burlington Ontario?\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
